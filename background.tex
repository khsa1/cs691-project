\section{Background and Related Work}
\label{sec:background}

The dominant FSM algorithm over the past decade has been gSpan\cite{gspan}. 
gSpan has been shown to give good performance relative to other existing
systems from both a computational time and memory consumption standpoint.
gSpan and algorithms like it are difficult to parallelise as it does a 
recursive depth-first search of its solution space, which is difficult to 
parallelize.  

Despite this, Buehrer et al. produces a shared-memory
parallel verson of a gSpan-like algorithm that achieved good speedup on a 
single NUMA system\cite{buehrer2005parallel}.  In this work, they cite 
memory contention as one of their main bottlenecks. It is our hope that
a distributed memory systems implementation using MPI may not suffer
from that issue.  More recently, Wang et al. parallelized gSpan using a 
GPU\cite{gspancuda}, again achieving good results.  However, the use of a 
GPU without any other higher-layer parallel framework limits the problem
size to the relatively small resources of a GPU. 

The success of these two previous implementations leads to two intersting
conclusions:
\begin{itemize}
	\item{That gSpan-like systems can be parallelized}
	\item{That even if a pure MPI implementation is not as performant, 
		there may be the possibility of using a hybrid approach
		that uses MPI to distribute work to these single-node 
		(or single GPU) implementations}
\end{itemize}

Recently, Vu and Alaghbad introduced ShaFEM\cite{shafem}, a new parallel FSM 
implementation that is not based upon gSpan.  This new algorithm may also
provide a good starting point for an MPI implementation (XXXjc: read paper
and fill this in more).

